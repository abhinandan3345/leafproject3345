{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ef85ac-940a-4d00-a26d-fb736404d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,Activation,MaxPooling2D,Dropout\n",
    "import seaborn as sns\n",
    "import visualkeras\n",
    "import keras_tuner \n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d2d0f-b018-48ba-b0e0-4131432a3b4e",
   "metadata": {},
   "source": [
    "# Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b5e7dee-4c3c-4b48-a6ae-ce3859751a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path(r'D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafProject\\Leaf Project Contents\\FINALDATASET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5c44e21-ff2e-442e-92d4-666a20f13e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = list(image_dir.glob(r'**\\*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ce424c3-c9f6-4071-a035-5fe68ce5fb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2146"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a2e5018-faeb-498c-aebb-3798b929edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(filepaths)):\n",
    "    \n",
    "    # list.append(i)\n",
    "\n",
    "    label=str(filepaths[i]).split('\\\\')[-2].split(\".\",1)[0]\n",
    "    labels.append(label)\n",
    "    \n",
    "    \n",
    "    # labels = pd.Series(labels, name='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed7055ac-713d-4a3d-bdbf-a5678f76107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amrud', 'Amrud', 'Amrud', 'Amrud', 'Amrud']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[80:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284eddba-bf56-4cc8-b176-251bb0d0bc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f357552a-fd11-48bd-8cb8-cc61fa080730",
   "metadata": {},
   "source": [
    "# Creating Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "470eeab4-b13f-4778-8903-fd2379515a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "df = pd.concat([filepaths, labels], axis=1)\n",
    "df = df.sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36b87a76-e386-4b48-92b2-fd55b4a20836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Neem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Chandni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Arhul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Jackfruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Arhul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filepath      Label\n",
       "0  D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...       Neem\n",
       "1  D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...    Chandni\n",
       "2  D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...      Arhul\n",
       "3  D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...  Jackfruit\n",
       "4  D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...      Arhul"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df=df\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61e19e8e-8d53-404f-82ce-f9114cd8a1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filepath    object\n",
       "Label       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cd0ca36-7ff5-4df1-8284-5e2dd0e826a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2146, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95e3e4e3-a5c3-4fd3-8675-898ab1df2b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2146</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...</td>\n",
       "      <td>Giloy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Filepath  Label\n",
       "count                                                2146   2146\n",
       "unique                                               2146     32\n",
       "top     D:\\CDAC SOFTWARE\\abhinandan\\CDAC\\Project\\LeafP...  Giloy\n",
       "freq                                                    1    192"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb50c3-1f94-44f0-94ce-12a0eb6300ed",
   "metadata": {},
   "source": [
    "# Details Of DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8073d4ca-0587-414a-bb56-26eb439790b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training set --\n",
      "\n",
      "Number of pictures: 2146\n",
      "\n",
      "Number of different labels: 32\n",
      "\n",
      "Labels: ['Neem', 'Chandni', 'Arhul', 'Jackfruit', 'Peepal', 'Tindora', 'Nimbu', 'Karanda', 'Drumstick', 'Mogra', 'Curry', 'Giloy', 'Chandan', 'Karanja', 'Amrud', 'Rasna', 'Anaar', 'Harive-Dantu', 'Pudina', 'Jamun', 'Jamaica Cherry', 'Paan', 'Ghrit Kumari', 'Malabar spinach', 'Rose Apple', 'Aam', 'Oleander', 'Phagoora', 'Parijata', 'Sarso', 'Tulsi', 'Mithi']\n"
     ]
    }
   ],
   "source": [
    "print('-- Training set --\\n')\n",
    "print(f'Number of pictures: {image_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(image_df.Label.unique())}\\n')\n",
    "print(f'Labels: {image_df.Label.unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67ffdc-c01c-44ab-98a1-3cc9206b5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44545300-4d33-4579-9b41-0e431cd92152",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5847c4-8e76-4592-b9c4-5832271a2aeb",
   "metadata": {},
   "source": [
    "## Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18d0b282-d17a-47ac-82e5-36ce0523fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DataGenerator = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    validation_split=0.2)\n",
    "\n",
    "test_DataGenerator = ImageDataGenerator(\n",
    "    rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92270963-408e-4bd8-ae1e-b86c80929387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(image_df, \n",
    "                                     test_size=0.2, \n",
    "                                     shuffle=True, \n",
    "                                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "139505b0-c354-46bb-861e-d120244a179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1373 validated image filenames belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "train_images = train_DataGenerator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    directory = image_dir,\n",
    "    target_size=(128, 128),\n",
    "    class_mode=\"categorical\",\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06d7a0ac-972e-4ed6-93dd-335a22e13091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 343 validated image filenames belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "val_images = train_DataGenerator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    directory = image_dir,\n",
    "    target_size=(128, 128),\n",
    "    class_mode=\"categorical\",\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9aec8a1-cee3-421c-9a2b-05bad863234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 430 validated image filenames belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "test_images = test_DataGenerator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    directory = image_dir,\n",
    "    target_size=(128, 128),\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03d5c445-4220-40ed-87fa-d11195c605cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 128, 3), (128, 128, 3))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.image_shape, train_images.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb690c-41be-4a95-86e5-d82ec303da7c",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning Of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72a630d9-ceb3-4d35-948b-c143d3d5c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):  # random search passes this hyperparameter() object \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Conv2D(hp.Int('input_units',\n",
    "                                min_value=32,\n",
    "                                max_value=256,\n",
    "                                step=32), (3, 3)))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):  # adding variation of layers.\n",
    "        model.add(Conv2D(hp.Int(f'conv_{i}_units',\n",
    "                                min_value=32,\n",
    "                                max_value=256,\n",
    "                                step=32), (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0a32a05-ffcd-4c42-9cb0-e91831ef493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=2,# how many variations on model?\n",
    "    overwrite=True,\n",
    "    executions_per_trial=2,  # how many trials per variation? (same model could perform differently)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "323e0196-5f35-47f0-b59e-8cd70fe3c042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "input_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "n_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': None}\n",
      "conv_0_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "695777b5-f294-470c-b083-c68ea224ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 35m 18s]\n",
      "val_accuracy: 0.7825581431388855\n",
      "\n",
      "Best val_accuracy So Far: 0.7825581431388855\n",
      "Total elapsed time: 01h 18m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=train_images,\n",
    "             epochs=10,\n",
    "             batch_size=64,\n",
    "             validation_data=(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d572262e-4bd8-459a-8efb-85c20cc0d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000214B17B66D0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 224\n",
      "n_layers: 1\n",
      "conv_0_units: 192\n",
      "conv_1_units: 96\n",
      "conv_2_units: 96\n",
      "Score: 0.7825581431388855\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 192\n",
      "n_layers: 3\n",
      "conv_0_units: 224\n",
      "conv_1_units: 32\n",
      "conv_2_units: 32\n",
      "Score: 0.39767443388700485\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab59e60-2720-4031-974f-b8022537fade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_units': 224,\n",
       " 'n_layers': 1,\n",
       " 'conv_0_units': 192,\n",
       " 'conv_1_units': 96,\n",
       " 'conv_2_units': 96}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d93f82ce-0b45-4315-93ba-e122f1fb2da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x214b8d57c40>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import keras_tuner as kt\n",
    "\n",
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6826e2b2-adac-4d44-963a-d5cc2d5d4762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 224)     6272      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 126, 126, 224)     0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 224)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 192)       387264    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 61, 61, 192)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 714432)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                22861856  \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,255,392\n",
      "Trainable params: 23,255,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = tuner.get_best_models()\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(input_shape=(None, 128,128,3))\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e0383-166d-457a-bf3d-cb387ab3206a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb61ef-80e4-4826-a9b4-2c27ba04a608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d99e57a-e902-4bed-89d2-44f7bf3ac53f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "827d6ab9-b0b1-4e8f-a310-d6f2dfd7a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sqequntial model object\n",
    "model = Sequential()\n",
    "\n",
    "# Add first Conv and pool layers\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3,3), \n",
    "                 activation = \"relu\", \n",
    "                 input_shape = (128, 128, 3),\n",
    "                 name= \"Input_Layer\"))\n",
    "model.add(MaxPool2D((2,2), name = \"Pooling_1\"))\n",
    "\n",
    "# 2nd Conv and pool layers\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3,3), \n",
    "                 activation = \"relu\", \n",
    "                 name= \"Conv_Layer\"))\n",
    "model.add(MaxPool2D((2,2), name = \"Pooling_2\"))\n",
    "# 3rd Conv and pool layers\n",
    "model.add(Conv2D(filters = 64, \n",
    "                kernel_size = (3,3), \n",
    "                activation = \"relu\", \n",
    "                name= \"Conv_Layer3\"))\n",
    "model.add(MaxPool2D((2,2), name = \"Pooling_3\"))\n",
    "#4th Conv and pool layers\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3,3), \n",
    "                 activation = \"relu\", \n",
    "                 name= \"Conv_Layer4\"))\n",
    "model.add(MaxPool2D((2,2), name = \"Pooling_4\"))\n",
    "\n",
    "# Flatten the conv layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add FC layers\n",
    "model.add(Dense(128, activation=\"relu\", name=\"Dense_1\")) # FC1\n",
    "model.add(Dense(64, activation=\"relu\", name=\"Dense_2\")) # FC2\n",
    "model.add(Dense(32, activation=\"softmax\", name=\"Output_Layer\")) # FC3 - Output FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10329c-0e53-4139-ad8a-6625a7f91d29",
   "metadata": {},
   "source": [
    "## Visualising The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06971486-e57e-4a32-9586-f48e1725922e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAADMCAYAAACxznh/AAAlxElEQVR4nO3deXwU9f3H8fdmk5AA4YZAOCQkhEstiAgKKIJVqdWiHCoePQSr1XqhttgLFQ/szwMRrAdF0arIKYqg0iglCMipgIYQDEcgIIGQgxybze7vDwySsFeunZ2d1/Px4FHY/c7MZ7/fSf2+852dsbndbrfCQOrK5Ro75jrdNipZUZERwT/+hoPasD1Xfc/tqY1bvgv68QEAAADUTKTRBdSH1JXLdcO4MfrPk0N18XnxQT/+zHnfKj0rXx3bNVazZs2DfnwAAAAANRf8JZR6VhmE5k4dbFgQenL2Nr0xZZDat45RhM0W9BoAAAAA1Jypw1AoBaHBfdvK5ZZsEYQhAAAAwAxMG4ZCLQhJktstVoYAAAAAkzBlGArFICRJbrdbtghTdikAAABgOaabuYdqEJJ08jI5VoYAAAAAUzBVGArlICSdXBniMjkAAADAHEwThkI9CEmSy8UNFAAAAACzMEUYMkMQklgZAgAAAMwk5MOQWYKQdPJuctxAAQAAADCHkJ65mykISZKLlSEAAADANEI2DJktCEk/rgwRhgAAAABTCMkwZMYgJP34nSFuoAAAAACYQsiFIbMGIanyOUMh16UAAAAAPAipmbuZg5DE3eQAAAAAMwmZMGT2ICRV3k2OMAQAAACYQUiEoXAIQhJ3kwMAAADMxPAwFC5BSJLcLlaGAAAAALMwNAyFUxCSTq4McQMFAAAAwBwMm7mHWxCSTn5niMvkAAAAAHMwJAyFYxCSKp8zxMoQAAAAYAZBn7mHaxCSfnzOEN8ZAgAAAEwhqGEonIOQxHOGAAAAADMJWhgK9yAk/ficIcIQAAAAYApBCUNWCELSj88Z4jI5AAAAwBQaPAxZJQhJlStD3EABAAAAMIMGnblbKQhJP95am5UhAAAAwBQaLAxZLQhJksvl5jtDAAAAgEk0SBiyYhCSeOgqAAAAYCb1HoasGoSkkzdQsPHQVQAAAMAUIutzZyERhF7fpjceDX4Qkri1NgAAAGAm9baMYfUgJP340FVuoAAAAACYQr2EoZAIQrONDUKS5GJlCAAAADCNOoehkAlCBnxHqDq3260InjMEAAAAmEKdZu4EoarcbsnGZXIAAACAKdQ6DBGEzuRyu7m1NgAAAGAStQpDBCHPuJscAAAAYB41DkMEIe+4mxwAAABgHjUKQwQh31wuycYNFAAAAABTCHjmThDyz+iVIYfDYdixAQAAALOxud1ut79G0597QjNeeEZtW8aobctGwairihPF5fo6I0+z/35ByAYhR7lL/a7+RAPO7a1+518Y9ONXVFTovaWLdeOE32nkyJFKTk5WfHw832ECAAAAvIj018DhcGj15yvUOb6JLjk/IRg1nWHFmn0qLC5XpD00J/aOcpdue3CDejRpqR5H3Cpe/mVQj1/qcmrJ4UwdLC/Wnj179PDDDyszM1MlJSXq1q2bkpKSlJycrG7duqlLly5q166dYmNjg1pjpfLycq1Yvkx33X2P4uLiDKkBAAAAkAJcGZryyB+lvLWacmf/YNR05vFf3qTlaXuVlV2ouY9fqPN7tzKkDk8qg5ByIvVyzxGKjrAH9fhFToeu3vqBujRqqpXHs3X6cBYUFGj37t3KzMzU7t27tWvXLq1OXaqSkhLFNYkOap2S5CivUEFRmWy2SO3POabo6ODXAAAAAFTyuzIUKtq2jNH941N069/W6p0nL1LfHi2NLimkgtD0xCHqs+W9Ku83a9ZM/fr1U79+/eRwODT2VyPU86xYzX3854qOCnKtxQ4Nm7BCNpuUV1Aquz24xwcAAACqM9Wtzy4b2F7PPtBP4//ypbZnHje0llALQr6OXxmEVLJXcx8fYlgQSkxooim/P0cxMdHavn17UGsAAAAAqjNVGJKkkYMT9Mw9fXXDI1/qu6x8Q2owaxB60+Ag9NrfBio6MkKtWzZTWlpaUOsAAAAAqjNdGJKkX17cUY/dcY7G/WmNMvYWBPXYBKHAnRGEok6ebq1bNdeaNWuCWgsAAABQnSnDkCRdN7yz/jbhbI390xrtzi4MyjEJQoHzFoQkqU2rZlq9erUCuHcHAAAA0GBMG4YkadzlXfTwr3tpzENpyjpY1KDHIggFzlcQkqQmjWPkdDq1b9++oNYFAAAAnM7UYUiSbhrZVfeO76ExD6Vp/+HiBjkGQShw/oKQJNlsNg0ZMoRL5QAAAGAo04chSfrN1d10x5hkXffgah08UlKv+zZTEJIU8kGo0uDBgwlDAAAAMFRYhCFJmnhtsn57TTdd9+BqHT5aWi/7NFMQcrgq1KldY1MEIelkGOKOcgAAADBS2IQhSfrD2O664YqzdN2Dq/VDXt0CkdmC0P15aerXs5UpgpAk9e3bV1lZWcrLywtChQAAAMCZwioMSdJ943voV8M6aexDaTqaX1arfZgxCDXqYtPcqUNNEYQkKSoqShdccIHWrl3bwBUCAAAAnoVdGJKkh27tqcsv7KCxD6cpr8BRo23NG4TMsSJ0Om6iAAAAACOFZRiy2Wx65He9NfS8dhr3pzTlFwUWiAhCgatrEJL43hAAAACMFZZhSDoZiKbcfrYG9GmtGyd/qcIT5T7bE4QCVx9BSJIGDRqkzZs3q6ysdpczAgAAAHURtmFIOhmInrjrXPVJaq7xf/lSJ0qcHtsRhAJXX0FIkuLi4pSSkqLNmzfXY4UAAABAYMI6DEknA9G0e/oqqVNT3fzXtSourRqICEKBq88gVGnIkCFcKgcAAABDhH0YkqSICJuevf88JbSN1a//vk6ljgpJBKGaaIggJPHwVQAAABjHEmFIkux2m158qL9aNY/Wb6esU+GJcoJQgBoqCEk/hSGXy1Vv+wQAAAACEWl0AcFkt9v00p/O18THv9IvbklTmcOl7o1b6M6dqUGvZVthrprY7Lq0WUetKjio5vZoxdmj1TwyWs3s0WoSESmbzRbWQUiSOnbsqGbNmmnnzp3q1atXve67JhwOh6Kjow07PgAAAILPUmFIkqIiIzRxVJJGpa3Wn5IGGlbHitw9uqVtijadOKICp0P5FQ4VVjiU73SooMKhMleFmkZEKb5jrPJLHLKn23T2mA8C2LMtgDbuANtJzooKNYqK0MuPDKj3IFSpT3KKHrtnkpKTkhpk//7s37tPm3Z9p8f+OU09evRQt27dFBMTY0gtAAAACB7LhSFJGtyvrSTpz8mDDKth2u71+nuXAV7fL3e7dMhxQlNLN+r263vpyos6B7G6n/zhyTRlZefr2kmrNePh/ko5q1m97j912XKlffGFbo7vqeKMw/W670CszsvW14W5Sj6rq2bPnq3MzEzt3btXbdu2VVJSkpKSktStWzclJycrPj5ebdq0CXqNoWLd2jR1S0rRpZdeanQpAAAA9cKSYcgMomwR6twoTk0cUeoc31S9k1oaUke7lrE6N6mpunRoomvu/5/uHd9Dv78uWRERga0s+ZK6bLmuHzNWr/YYoYtaJNRDtTXzevY2ZRQf14C4dhp2xUhNfeUlSVJFRYX279+v3bt3n/rz+qsz9dX6dUpo1yTodYaC3Lxi5RU41D2lhxYtWqyePXsaXRIAAECdEYbgl81m02+u7qZL+rfTPc9s0oovczT9wf7qmlD7YFAZhF7uPsywIPTcvs2amXSx1hdWXZGy2+3q2rWrunbtqhEjRih15XLNee0lvTftEl18XnzQazXazHnf6snZ2xTbyK5Ro67V0KFDdeedd2ry5MmKjY01ujwAAIBas8zd5FB3iQlNteTZi3XFhR008o9faO5HWXK73TXeT35uQcgEoYFxvsNN6srlumHcGM2dOtjSQeiNKYOU0K6pbr75Zm3dulU7d+7UOeecoxUrVhhdIgAAQK0RhlAjdrtNfxjbXYufHaq3lmXpxke+VE5uScDb78os0sZN6QQhEzg9CA3u2/bU6x07dtS8efP00ksv6e6779a4ceN04MABAysFAACoHcIQaqVn12b6eMYw9e/VSiN+n6oF/93nd5Vo9fpcvbZwt17reRlBKMR5C0Knu/LKK7Vt2zb16tVLffv21fTp0+V0OoNcaWjJyMgwugRToJ8CQz+ZE+NmToxbYMzSTzWpk+8ModaiIiP00K29dPmg9rp72iZ9nJajZ+7tqzYtGp3RdvX6XE189Cu90oMgFOoCCUKVYmNj9eijj2r8+PG6ddwNev7vjynGot8jcjnKdaAgTwmJZ/l8ZpXTcULlZUWKiTnz58QKXBXlys7JU4eOXX33U2GxHAWFamTR29wHej4dd5Qqr6yYxwGECEeFU8U5R5TUsbPvcSsu0/HiEsYtRJQ5nSrOPaykLoybL2bppzKnU2VFRdqxYa1SUlL8ticMoc5+ltJSn718qaa98a2G3f5fPXNvX/1i8E+BpzII/SvFuLvGEYQCU5MgdLoDmd9r984MPZM0WN1imzdghaHpvZx0vZn/neLjWuiDD7w/D2zD+tWa9MD9mjn5AiV3jgtihaHhzQ8z9fqiXWrXtrnPfvpqVZom3X+f/pk0hPPJRz+lrv9S9z3wgCr+Ol4lXdoFsUJ49MGX0oL/qXHbVj7H7fM1a3Xv/Q/IdvVElbfuEMQC4UnFps/l2viZGrdk3HwxSz9V1tm0TbuAgpBEGEI9iYm26x+3n6ORFyXo7mc26uO0g3rirnP1zY4CgpBJ1DYIVd4Z8F8plxoyxkZ7PXub3jucoT91PE/zHAfVu3dvj+1SVy7XQ5Me0NtPDLHs+fXWR7s15fdn642Pj3jvp2XL9dADD+iVlOGcTz7Op6UrP9X9D05SxbTbpP6B/QcfDejdVGnpl9IffqmYj7Z4H7cVn+m+SZMUMfY+RXT13AbBU772Y7m2fi4NG6eYnWsYNy/M0k+n1xm9c03A2xGGUK8uOLu1Pn9lhB57bbsuu/UL2Rx2dWvUXLNzdmh2zo6g1nLCWa7NBYc1tk2ycstL9L/8g2oeGa04e7Sa26MVZ49SdIRdknTwh2yCUB2CkFE3xDDa6WG7VWQjzTt60GM7gvZP51frFo30xsdHPLbjfArsfFq68lONvn6snE/9jiAUCt5NlV5dJk39jdSiifTRFo/Nlq74TKPHjZVt9L2WnFCHmvK1H8u1aoE06m6pcZzkZfJs9XEzSz8FWqcnhCHUuyaxkXr8znO1/9uNKsmVBrfqaEgdaUez1TY6VmWuCn16PFuFFQ7lOx0n/7fCocKKckXZIhRhl5qszlRkpE0THv3Sxx7dkur+sNnQ41bhiXIN7ddWR46XKSe3RB3a+P/eDxPXqquOu0qOe2xHEKoatNP3FHhsx/kU2PlEEAoxpwehfklS1iGPzYyeKKKqKhPnLj2lXM93RLX6uJmlnwKt0xvCEBpEdFSE+vZuoYjtTfTn5EGG1PC01mlLXo4eO+sCj++73W4Vu5x6KGuNMmMK9dGMK4JcYWjI3F+giVNWaUCfVlqcul9/fnGrmjaO1MCz22jg2a018JzW6l7t+y1MXAO7/JIgFNiKI+dTYOcTQSjEVA9CXhg9UURVZ0ycvbD6uJmlnwKt0xfCECzLZrOpiT1KbaMaK7tRsXontTS6JEO45VZ0tF13jUvRXeNOhsTM/UVaty1X67cf1Yx5GSooKldZuUtz5sxRi9gmeuKpJzSxw9kqdDr0Se4eoz9CUKUe3a/FRzL1ty7nq1Vko1O/wd9bWqgyZ7l27Dh5Oeh/P1uuP0+erD/e0FMFRQ599L/9BlYdfJ+sPaD3P8nSk3f/TK1bNDq1IpR1oEhlZT/108plyzX5L5M5n/ycTx9+9okemTxZ7puGS4Ul0hdfG1g19OUOaflX0r3Xnrw0rnJF6ECunGWOn8ZtxSd6ZPIjihj0C6n0hCrSNxhYNJwZW6VtadJlN528lKpyBSHvBznLGbdKZumniu93yP31KunaP9Y6CEmEIQDV2Gw2de8Sp+5d4nTLVYmSpEO5Jbri7lXKyMjQV5/8V22jGmv5sb1afmyvwdUGX9aJ42oZ2UizD39X5XWHu0I5zlJdf/31Kisr04n8g2rfOlZLV2Vr6arsqjsJ1ysuT5O5L1+tmkdr1vzMKq+XOSp0MLfsVD8V7s/hfArgfNqdnyt362aypX4tpVYNQhY4nUKOe9/hkyFo3hdV33A4deJIwalx+/6HY7I1bSF3+ga5LTahDklHD0qxcdKGT6q+7izXiaI8xq2SSfrJfeyQNGxcnYKQRBgCEID2bWIV1yRaTz31lEZ/dYXmJA1Tz6atjS7LEAPS5uqh9udqeItOVV7fVXJcDxzdqu3bt0uS+vTopPlPX2TZFceev5qnv07opSsvrHrZW/qeAt3+1Len+qlXh86cTwGcT617dFPhsxNkSzLmO5ioqvyqP8t9+0hpcJ+qb2QdUvOpC06NW5uuySofc7/s8V0MqBLV5f/zTrmGjJa69636Ru4BNf/s34zbj8zST/n/vFOuZm3qvJ+IeqgFAAAAAEyHMAQAAADAkghDAAAAACyJMAQAAADAkghDAAAAACyJMAQAAADAkghDAAAAACyJMAQAAADAkghDAAAAACyJMAQAAADAkghDAAAAACwp0ugCAMBU3J5fziotVE5+nvr06SNJys/PD2JR5rE7u0g5P/zUTwVW76canE/89jJ0uL2Mm/YfUdHh3Crj1jh4ZaG2jh1W0THGzS+z9NOxw3JWVATcnDAEADVhO/OldYWH9LcDG/X8Cy9o4MWDJUnXXTMiyIWFvrStRzTp+a/1/PMv6IJBQyRJ1w67zOCqDBbg+XTRNSNVHOTS4J1NHnLs5kzZ/2+Rnn/+BQ0bdKEkafAVV8kV7OJQM/vSZf9s7slxGzxIEuPmkVn66cc6X3vrrYA3IQwB8Prb6dOVOys0Z84cFRYWNnw9JrKu8JDu27dW8xcv1PCRV5563W63G1hV6EnbekQTp27S+wsWafiIn/opgn6qgvPJJKqH2M2Zinz0HS1ZsFBXjfj5qZftdnvoTRbxk33pivzoFS1ZtEBXXc64eWWWfvJSpz+susPyHO4Kub1e82ANbj9pKG3rER0+WqpDhw4pJiYmSFWFvsqJ6/uLqk5cUVVlEJo3f2GVIISqOJ9MqjIIzV9QJQghxFVOnBfOr9HE2XLM0k91qJMwBEtbV3hInxzfL5vNw7UqFuLr81dOZJd+uFRvvfWWoqKiglhZ6GLiGhiCUGA4n0yKIGROZpngG80s/VTHOrlMDpZVOfl44aUZeu7ZvxhdTkhiIuvZ1yeOat7x75m4+rElPU9zl+3n/PGD88mkvtunyA83EITMJud7RW5fHfoTfKOZpZ/qoU7CECzp9N/CxnfpLD1rdEWhx1sQcpaXK6vYuncAK3U69e8j6Zr50kuK79JZO3bs8NjOUeZQ5v4Cv5cghquSUqdmzd+tmTNfUnx7H/3kcHA+BXI+ORxy7/vB+13MEFTuEoc0b5Wmz5ylru0TfIxbmVxHcwL6XiYanru8TNr4iabPmqmuHRk3b0zTT85y2TZ/piUfflinwEYYguVUvxzF2w+5lXkKQhs2bNC0adNks9n0j/0bFGnRy+VOuJ1qHR+v516aIb00w2u7mJhoPTx9q3X7qaRCrdvE67nnZ0jy0U/R0ZxPAZxPUY2iFfHsIsv2U6g5XupQ+zbxmvX8dM3SdK/toqMbyfnZ24qKYroVCmyOMnWIb6dZ06dr1nTGzRuz9FOEq0Ivv/JKnVeurDnKsCyuy/fv9CB06fArtHLlSk2bNk0ZGRmaNGmS3nzzTTVp0sToMg2TkZGhlJQUo8sIefRTYOgnc2LczIlxC4xZ+qm+6iQMwTIIQv5VBqF33ntfecdPaODAgTpx4oQefvhh3XjjjYqOjja6RMOZ4T8QoYB+Cgz9ZE6MmzkxboExSz/VV52EIVgCQci/tK1HNOHxjfrthDt0z70Pqnnz5nrkkUd0zTXXKCKCG08CAIDwQxhC2AskCFn9OUPOCpdu/us6NWnaQt98s0OzZs3SsGHDLH/LcQAAEN4IQwhrR8tLTwWhS6+8QocPH1ZWVlaVP998840y9x1X72sXWHLyX1rmVF5BmQZdOFjPPvu8+vfvb3RJAAAAQUEYQoM5ll+mnceO6+nMdYYcf/XRbO0qOa7e5/XVPQ89qD1jxyg2NlaJiYmn/vTv31/XXHONWrVqpZYtWxpSp9H279+vQznZ+vVvbjO6FAAAgKAiDKFB/JBXqhVbD2h4nw5ytTkhSXK53Fq95YgU2VRXXHlVg9fQdnOUft72PN0y8bZT4ScuLq7Bj2s2vXv3NroEAAAAQxCGUO9cLrfufnqjrr+qsyb/ro8kyVHu0oTHv1L7c3po/pKVQbkrmcPh4O5nAAAA8IpbRKHezZq/S8WlFXro170k/RSEopolBS0ISSIIAQAAwCdWhlCvNn57TLPm79KnMy9VpD3CsCAEAAAA+MPKEOpNfpFDdzz5lf55Xz91im9MEAIAAEBIIwyhXrjdbk16bosuG9heVw1JIAgBAAAg5BGGUC/e/niPMrOLNOX35xCEAAAAYAp8Zwh1lr6nQE/++1stff5iRdhsBCEAAACYAitDqJPiUqdun/qV/j7xbJ3VoQlBCAAAAKZBGEKd/P3lberTrbmuG96JIAQAAABT4TK5EOc2ugAfPvgiW//b8oOWvzhME6duIAgBAADAVAhDIczhqlC2o8joMjzam3NCk2d8rTcfG6T7n9tCEAIAAIDpcJlciHK4KnRvVpoiYhoZXcoZyp0u3fHEBt11fXfNmLeLIAQAAABTYmUoBFUGoZheiUpsYzO6nDM8PedbtWgWpXXbjiq6eTJBCAAAAKbEylCIOT0ILfxipSJsoRWGPt94WAv+u19utwhCAAAAMDXCUAipHoRCLWT8cKxU9zyzSZ3iG6tJ6xSCEAAAAEyNMBQiQj0IuVxu3fnUBjVtHKn2nXsThAAAAGB6hKEQEOpBSJKmv7tT2zLz1bN3X4IQAAAAwgI3UDCYGYJQ9g/F+mj1QV0y+HwtXPrfkKwRAAAAqClWhgwUSBByuY197KqjvEIffHFAfc/pqY8+WU0QAgAAQNhgZchApwchl8ul9PR0ZWVlKSsrS3v27FFWVpZ27crUfevzNeVfWwypMSv7uNrHt9LqtVsIQgAAAAgrlgxDLtfJ1ZanM9cZWsd3kQ51jXApMTFRx44dU+fOnZWYmKjExER17dpV/fv3V9xvf6uzzjpLNoNusf3hB0t028TbCUIAAAAIO5YMQ5+szVGf+JZy9i5SRET9hIwfjpVqzdZcDb/sMsXHd/DbvnfJQT3w2D/UvXt3JSYmKiEhQXa7vV5qqU+9e/c2ugQAAACgQVguDLndbj33droevKOHfnlxx3rZZ9rWI5o4dZPmLV6o4SOuDGibv07/P1ZbAAAAAANZ7gYKqRsOq6zcpV8MSaiX/Z0KQvMDD0KSCEIAAACAwSwVhtxut559O133je9RL5fH1TYIAQAAADCepcLQ6i1HlFfg0K8u6VTnfRGEAAAAAHOzVBh67sdVIbu9bqtCBCEAAADA/CwThtZty9WBH0p03fDOddoPQQgAAAAID5YJQ8++na57bkxRVGTtPzJBCAAAAAgflghDm747psx9hbr+8rNqvQ+CEAAAABBeLBGGnns7XX+8IUXRUbX7uAQhAAAAIPyEfRj6Ztdxbcs8rvEju9Zqe4IQAAAAEJ7CPgw99590/WFsd8VE22u8LUEIAAAACF9hHYa+/T5fG3Yc1a2/TKzxtgQhAAAAILyFdRh64Z2dumN0dzWOiazRdgQhAAAAIPyFbRjata9Qq7cc0W+vqdmqEEEIAAAAsIawDUMvvLNTE65NUtPGUQFvQxACAAAArCMsw1DWwSKtXH9IE0Z1C3gbghAAAABgLWEZhma8l6HfXJ2o5k2jA2pPEAIAAACsJ+zCUPbhYn30v4O6fXRyQO0JQgAAAIA1hV0YmjEvQzf94iy1bt7Ib1uCEAAAAGBdNbvndIg7lFuixanZSptzmd+2BCEAAADA2sJqZWjm+7s07vIuatcyxmc7ghAAAACAsAlDR/JKNe/TfbprXHef7QhCAAAAAKQwCkMvL8jUqEs7qUObWK9tCEIAAAAAKoVFGDpWUKa3l+3RH29I8dqGIAQAAADgdGERhl5duFtXDU1Q5/jGHt8nCAEAAACozvRhKL/IoTlLv9c9N3peFSIIAQAAAPDE9GHo9SXf67KB7ZWY0PSM9whCAAAAALwxdRgqKi7X64t3677xPc54jyAEAAAAwBdTh6E5S7M0tF9bde8SV+V1ghAAAAAAf0wbhk6UOPWvhbvOWBUiCAEAAAAIhGnD0FvLsjSgT2v17tb81GsEIQAAAACBMmUYKnVUaOb7u/TATT1PvUYQAgAAAFATpgxD7yzfo3O7t9C53VtIIggBAAAAqDnThSFHuUsz3svQAzefXBUiCAEAAACoDdOFoXmf7lVylzj179WKIAQAAACg1kwVhsqdLr34boYm3dyTIAQAAACgTkwVhhal7lfHdrFyVrgJQgAAAADqxDRhyO1264V3duryCzsQhAAAAADUWaTRBQQqJ7dEkfYIvfhept5fsIggBAAAAKBOTBGG3G630rPyFRUVqcVLlhCEAAAAANRZQGGoqKhImzYe1JSXG7oczz74fI9cLrcWLV5MEAIAAABQL2xut9ttdBEAAAAAEGymuYECAAAAANQnwhAAAAAASyIMAQAAALAkwhAAAAAASyIMAQAAALAkwhAAAAAASyIMAQAAALAkwhAAAAAASyIMAQAAALCkyNpslLpyucaOuU63jUpWVKTvPJW64aC2pB/TLTf8Uq+9+UGtijSb1GXLNea60bo5voeibL77Z3Vetr4pOqqbRl6t2csWB6lCnG7pyk917djRcl07WIry8yOx/jvpu30accNorXzzveAUGKKWrvhM144eLVv/EZLdd79V7P5GysnSiFFjtHL+O0GqEAAAwLcah6HUlct1w7gx+s+TQ3XxefE+286c963Ss/J14Tlt1KFDx1oXaSapy5br+jFj9WqP4bqoRYLPtq9nb1NG8XGd37StOnSyRv+EmqUrP9Xo68fKNW2C1D/Fd+N3U6Xvc6Rzu6lTB99jG+6WrvhMY8aNlX3c/Yro2ttn2/K1H0tHsqVOKeqUYO1+AwAAoaVGl8lVBqG5UwcHFISenL1Nb0wZpAF9WtepSLOoDEIvdx8WUBB6bt9mzUy6WOc1bRukCnG6yiDkfOp3gQWhV5dJU38jndM1GOWFrMogpNH3BhSEXKsWSKPuljomB6lCAACAwAQchmobhAb3tcZEv7ZBaGCc775Ew6h1EOqXFJT6QlWtg1CXnkGqEAAAIHABXyZ3++9uUFLnppr1frpmvZ/utV1xiVPbMvNCOgjZbLZTf3e73fWyzwnjblRidFPNztmh2Tk7vLYrrnDqu6KjYROETu9L6cz+rHy/Lv3cEOM16rZb5O7cRnr3i5N/vCkpk3Zl13sQ8tdvvrZpiD4O1LU33yq1aC/b+uVyrV/utZ3bUSbX4b0EIQAAENICDkOd45vokvP9X++/auNB9e/Vuk5BqDYTxZru2+12y2aznXGs2h6zY6M4DW7l/3s/a44d0M+atql1ELLZbFVqq/7vQPdRXW37uHK7+urH6hpqvNS+lewD/E/SXRvS5e7TtdZByFtf++u3UBXRvK0ik8722865e7uUkCRXLYNQQ/5/AAAAQKWAw9Al5ydoyp39/bab8rK0ccfhWhdUffLr6b3q73v7u6f3PE2qPG1f05AxuFVH/Tl5kN92T2eu05a8nID360llbf76x9Nr1bf19Hm9rcTUZIXG02TW23hV38bb+772VdPxihjQU/a7RvlvOHOJKrZ/H/B+valJbf7G0du5Xr3PAv15qYnIpLMV+/Mb/bYr0bty7Muo0b49CeT8DPS8On0bT/sBAADWE9LPGfL0G/TT/+3v774E89KiYPA2EfY1KazJfnxNtD3xFDy9jVf1/fsLSmbka1WrejvJ9zie3rfeXq/Jz4uZ1PRzVQ88vvoHAABYT0iHoboIJBSYdWItBfbb7urvVf/76e1DuS/MUKM/3lYlPbWT6m9yHq6T/EA/V/Wfe1+rqQAAwHpq9dDVYGmISXB9XCpkNE+XuwV6eWH1lbNQ/+zhMF414WscvbXz9HqlcO2rmnwuf6tFAADAuoKyMlTudAXc1tuKjq+VHn/78tTW03ctarr/+lLuDrx//Kn+WfxdLudvhclf/1f/bXvlvz297ou/FZFQGi+VV9R4E08B1dtYVW/v79I5T69XH5+QWAGpcNZ4E1+fNZDP5aldyPYPAAAIugZfGUrbekRzP9qjJR+ODngbbxNbb1+mD/R9X/s2yrrCQ5p3dLc+GPtiQO2rTwp9BT5v29a2XSD9W5N9+/pOUW333+A2Zyriw3Uav/SvATWvSV/W9/5q8/PSYPalK+KbVRr/5IMBNfdVW00+V033AwAArKVBw1Da1iOaOHWT5i9aoqEXj2jIQ5nSusJDum/fWi34YLGGXjbc6HIMZYrLuzZnKvLRd7Rk4WJdfvEwo6sxj33pivzoFS1ZvEiXXzrM6GoAAABOabAwVBmE5s1fqOEjrmyow5hWZRB6f9FCDR9J/4Rk+DldZRCav0BXjfi50dWYR2UQWjhfV11OvwEAgNDSIN8Zyj1eShDy4Wh5KUHITPKKCEK1UVxIEAIAACHN5g7wV/KXDkjQJecn+G23auNB7T98Qq/OmWepIDS0VWcNbtXRb7s1xw7oQFmhXl/wHkHIQBEDeyliQE+/7Vwb0qWcY/pwztsEIUlRSecqMulsv+2cu7fLlX9ES9+ZSxACAAAhK+AwBAAAAADhJGwfugoAAAAAvhCGAAAAAFgSYQgAAACAJRGGAAAAAFgSYQgAAACAJRGGAAAAAFgSYQgAAACAJRGGAAAAAFgSYQgAAACAJf0/HHyQAQ+CHSEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=835x204 at 0x1BEAF28BBB0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model, scale_xy=1,scale_z=1,legend=True,max_z=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d9848-a885-4263-ae6c-f553f8c1f199",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ffc30ea-fada-42e4-ac1a-f08203de189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (Conv2D)        (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " Pooling_1 (MaxPooling2D)    (None, 63, 63, 32)        0         \n",
      "                                                                 \n",
      " Conv_Layer (Conv2D)         (None, 61, 61, 32)        9248      \n",
      "                                                                 \n",
      " Pooling_2 (MaxPooling2D)    (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " Conv_Layer3 (Conv2D)        (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " Pooling_3 (MaxPooling2D)    (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " Conv_Layer4 (Conv2D)        (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " Pooling_4 (MaxPooling2D)    (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               295040    \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Output_Layer (Dense)        (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 370,944\n",
      "Trainable params: 370,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13c4a4-351e-403b-8b9f-f7d41a176238",
   "metadata": {},
   "source": [
    "# Compile / Optimizing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c891e61-9a4a-4444-8521-ead42f052fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = tf.keras.optimizers.Adam( learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee618b7d-1883-4bb6-b820-9194ae41a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", # with RMS accuracy is less around 20 percent with 2 epoc\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cb544-24f1-4eda-963b-2df8ce7508b6",
   "metadata": {},
   "source": [
    "# Building/Train a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28b0d9-f62d-47e6-9007-26657b241fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "43/43 [==============================] - 104s 2s/step - loss: 3.3455 - accuracy: 0.1085 - val_loss: 2.9983 - val_accuracy: 0.2595\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 41s 961ms/step - loss: 2.2993 - accuracy: 0.3671 - val_loss: 2.0586 - val_accuracy: 0.4344\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 45s 1s/step - loss: 1.4041 - accuracy: 0.5732 - val_loss: 1.4499 - val_accuracy: 0.5714\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 42s 967ms/step - loss: 1.0532 - accuracy: 0.6744 - val_loss: 1.3683 - val_accuracy: 0.6414\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 46s 1s/step - loss: 0.9127 - accuracy: 0.7283 - val_loss: 1.1397 - val_accuracy: 0.6968\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 42s 983ms/step - loss: 0.6927 - accuracy: 0.7910 - val_loss: 0.9656 - val_accuracy: 0.7376\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 42s 969ms/step - loss: 0.5497 - accuracy: 0.8245 - val_loss: 1.0237 - val_accuracy: 0.7143\n",
      "Epoch 8/30\n",
      "43/43 [==============================] - 43s 995ms/step - loss: 0.4642 - accuracy: 0.8449 - val_loss: 0.8645 - val_accuracy: 0.7668\n",
      "Epoch 9/30\n",
      "43/43 [==============================] - 44s 1s/step - loss: 0.3768 - accuracy: 0.8776 - val_loss: 1.0636 - val_accuracy: 0.7259\n",
      "Epoch 10/30\n",
      "43/43 [==============================] - 47s 1s/step - loss: 0.2755 - accuracy: 0.9148 - val_loss: 0.8424 - val_accuracy: 0.7901\n",
      "Epoch 11/30\n",
      "43/43 [==============================] - 49s 1s/step - loss: 0.2692 - accuracy: 0.9111 - val_loss: 0.9835 - val_accuracy: 0.7697\n",
      "Epoch 12/30\n",
      "43/43 [==============================] - 49s 1s/step - loss: 0.2093 - accuracy: 0.9228 - val_loss: 0.8699 - val_accuracy: 0.7843\n",
      "Epoch 13/30\n",
      "43/43 [==============================] - 113s 3s/step - loss: 0.1988 - accuracy: 0.9366 - val_loss: 1.0881 - val_accuracy: 0.7668\n",
      "Epoch 14/30\n",
      "43/43 [==============================] - 51s 1s/step - loss: 0.1283 - accuracy: 0.9592 - val_loss: 1.3509 - val_accuracy: 0.7405\n",
      "Epoch 15/30\n",
      "43/43 [==============================] - 47s 1s/step - loss: 0.1892 - accuracy: 0.9315 - val_loss: 1.1141 - val_accuracy: 0.7609\n",
      "Epoch 16/30\n",
      "43/43 [==============================] - 68s 2s/step - loss: 0.1429 - accuracy: 0.9534 - val_loss: 1.1043 - val_accuracy: 0.7843\n",
      "Epoch 17/30\n",
      "43/43 [==============================] - 64s 1s/step - loss: 0.1056 - accuracy: 0.9650 - val_loss: 1.0328 - val_accuracy: 0.7697\n",
      "Epoch 18/30\n",
      "43/43 [==============================] - 56s 1s/step - loss: 0.0847 - accuracy: 0.9745 - val_loss: 1.0266 - val_accuracy: 0.7930\n",
      "Epoch 19/30\n",
      "43/43 [==============================] - 53s 1s/step - loss: 0.0688 - accuracy: 0.9811 - val_loss: 1.1662 - val_accuracy: 0.7638\n",
      "Epoch 20/30\n",
      "43/43 [==============================] - 64s 1s/step - loss: 0.0789 - accuracy: 0.9767 - val_loss: 0.9750 - val_accuracy: 0.8076\n",
      "Epoch 21/30\n",
      "43/43 [==============================] - 61s 1s/step - loss: 0.0725 - accuracy: 0.9789 - val_loss: 1.0417 - val_accuracy: 0.7988\n",
      "Epoch 22/30\n",
      "43/43 [==============================] - 63s 1s/step - loss: 0.0177 - accuracy: 0.9978 - val_loss: 1.0074 - val_accuracy: 0.8222\n",
      "Epoch 23/30\n",
      "43/43 [==============================] - 58s 1s/step - loss: 0.0552 - accuracy: 0.9832 - val_loss: 1.0923 - val_accuracy: 0.8105\n",
      "Epoch 24/30\n",
      " 6/43 [===>..........................] - ETA: 41s - loss: 0.0436 - accuracy: 0.9948"
     ]
    }
   ],
   "source": [
    "# Build/Train the model using CNN\n",
    "leaf_model = model.fit(\n",
    "    train_images,\n",
    "    epochs = 30,\n",
    "    validation_data=val_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9fa6f-ca35-4314-8b9c-79533fe7e699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c57e48f9-eab4-4ea4-89b9-6eb4fe68dfeb",
   "metadata": {},
   "source": [
    " * 30 epoches are required to get the maximum accuracy *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af0865-828a-4ff7-bf37-e080d289c52c",
   "metadata": {},
   "source": [
    "## Training Model Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04824758-5e4e-4a1d-979a-91199c0c15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(leaf_model.history['accuracy'], label=\"train_accuracy\")\n",
    "plt.plot(leaf_model.history['val_accuracy'], label=\"val_accuracy\")\n",
    "\n",
    "plt.plot(leaf_model.history['loss'], label=\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a1cbb-5468-4aaa-bd6a-b970cc1d3acb",
   "metadata": {},
   "source": [
    "# Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae9275-0e3d-48e0-9416-1b585b44141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/leaf_model4.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb89c1b-cc99-4180-9544-68c16062ba91",
   "metadata": {},
   "source": [
    "# Evaluating/Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697c524-a7fb-4b9a-a1be-ffc5de0aead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efff856-f510-4db0-a0f9-a0cd23b24c1d",
   "metadata": {},
   "source": [
    "*  The train accuracy is 100 percent and test accuracy is 83.37 with 30 epoches*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cdaa2-8af4-403b-b1d6-fb729df28ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_images.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e9672-8f70-4320-af8f-38f4ce956313",
   "metadata": {},
   "source": [
    "# Checking The Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd261055-16a7-4b1d-a97d-765f2657dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pobabilities = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66e361-4dd5-4e4d-9273-aa87d478e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pobabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924d1fa-6732-419f-bb27-cc8b158484d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [np.argmax(prob) for prob in prediction_pobabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a62da-bd0a-42e1-b257-c577c5fb8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5de09e-0de9-46c1-ae7d-443d1526587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_images.labels, test_predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc938b3-d717-4198-a6eb-87ae89bf89b2",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f46560-facc-44e6-80f0-18972db049c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_images.labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc1bc9-bc7a-4b50-a1ab-3e46bf5c2593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f5630-adc5-4843-af63-c3af4a5fc966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c4aee-6af8-4885-a74b-6174209ab6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e33ac-5e9e-4500-9333-593d4ce8ea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
